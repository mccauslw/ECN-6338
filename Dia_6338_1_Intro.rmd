---
title: "ECN 6338 Cours 1"
subtitle: "Introduction"
author: "William McCausland"
date: "`r Sys.Date()`"
output: beamer_presentation
urlcolor: blue
---

## Quelques observations

* Sciences économiques et la recherche des implications des modèles.
* La difficulté d'exprimer ces implications en forme analytique.
* L'importance de l'optimisation et des espérances dans les problèmes d'agents économiques.
* L'importance de l'optimisation et des espérances dans les problèmes économétriques.
* L'utilité de l'analyse numérique en microéonomie, macroéconomie et économétrie.

## Deux opérations numériques importante en économie

### L'optimisation

Par les agents des modèles économiques

* maximisation de l'utilité (choix des actions dans un jeu, des quantités de consommation)
* maximisation du profit (choix des quantités de production)

Par les économètres

* estimation économétrique par maximum de vraisemblance
* plus en général, [extremum estimation](https://en.wikipedia.org/wiki/Extremum_estimator)

### L'intégration (souvent, l'évaluation d'une espérance)

Par les agents

* évaluation de l'espérance de l'utilité, du profit

Par les économètres

* simulations Monte Carlo des estimateurs
* simulation bootstrap
* inférence bayésienne

## D'autres opérations

D'autres opérations numériques jouent souvent un rôle de soutien

* Résolution de systèmes d'équation
    - reliée à l'optimisation, à la recherche des racines
    - recherche d'un équilibre
* Approximation de fonctions
* Résolution d'équations différentielles
* Simulation de variables aléatoires
    - pour l'intégration (méthodes Monte Carlo)
    - pour l'optimisation (recuit simulé)

## Ce cours, relatif au livre classique de Judd

Relatif au livre de Judd, je mets un accent sur

* l'économétrie (cependant, ce n'est pas un cours d'économétrie)
    - exemples dans le domaine de choix discret
    - maximum de vraisemblance
    - inférence bayésienne
* la simulation
    - intégration par simulation (utile en grandes dimensions)
    - optimisation par recuit simulé
    - applications en inférence bayésienne

Je mets moins d'emphase sur l'optimisation dynamique.
Le but ici est de présenter les cas les plus simples et de vous guider vers la matière plus avancée.

## Documents et Communication

### Site Github du cours

1. Diapos (code source, pdf)
1. Démonstrations
1. Lectures, exercices
1. Devoirs avec computation
1. Liens vers les enregistrements des cours à distance (README.md)

### Site StudiUM du cours

1. Messages
1. Forums (possiblement)
1. Documents avec droit d'auteur
1. Chargement des devoirs

## Logiciels (pour les travaux pratiques, votre choix)

### R

* graticiel, accent sur la statistique, beaucoup d'applications
* utilisé pour les démonstrations du cours

### Python

* graticiel, général, beaucoup d'applications

### Julia

* graticiel, général, moins utilisé que les autres
* très rapide, élégant

### Matlab

* commercial mais disponible à l'université, général, beaucoup d'applications
* toujours populaire mais son importance diminue en faveur de R et python

## Notation pour les dérivées multivariées

* Soit $x$ un vecteur $n \times 1$, $y = f(x)$ un vecteur $m \times 1$.
* La *matrice jacobienne* ($m \times n$) contient toutes les dérivées de première ordre:
\[
  f_x = \frac{\partial y}{\partial x}, \qquad \mbox{où} \qquad
  \left[ \frac{\partial y}{\partial x} \right]_{ij} = 
  \frac{\partial y_i}{\partial x_j}.
\]
* Le *gradient* est un cas spécial du Jacobien où $y$ est scalaire, un vecteur ligne $1 \times n$.
* La *matrice hessienne* ($n\times n$) contient toutes les dérivées de deuxième ordre pour $y$ scalaire:
\[
  f_{xx} = \frac{\partial}{\partial x}
  \left( \frac{\partial y}{\partial x} \right)^\top
  = \frac{\partial^2 y}{\partial x \partial x^\top}.
\]
* La notation ci-haut suit la convention "numerator layout"
[ici](https://en.wikipedia.org/wiki/Matrix_calculus)

## Quelques propriétés des dérivées multivariées

À la même [page](https://en.wikipedia.org/wiki/Matrix_calculus) il y a des tableaux de propriétés, telles que :

* Pour une matrice constante $A$, $m \times n$,
\[
  \frac{\partial Ax}{\partial x} = A.
\]
* Règle du produit : pour les vecteurs $u(x)$ et $v(x)$, $m \times 1$,
\[
  \frac{\partial u^\top v}{\partial x}
  = u^\top \frac{\partial v}{\partial x} + v^\top \frac{\partial u}{\partial x}.
\]
* Règle de la chaîne, des fonctions composées : pour $z = g(y)$,
\[
  \frac{\partial z}{\partial x} =
  \frac{\partial g(y)}{\partial y} \frac{\partial y}{\partial x}.
\]

## Analyse de l'erreur

Deux sources d'erreur numérique :

- Précision finie des nombres réels
- Troncation de calculs infinis
    
Les erreurs se propagent à travers les computations.

## La représentation virgule flottante

L'ordinateur représente un nombre réel $x$ comme
\[
  x = m \times 2^e,
\]
où

- $m \in \{\ldots,-1,0,1,\ldots\}$ est la mantisse et
- $e \in \{\ldots,-1,0,1,\ldots\}$ est l'exposant.

Le nombre de bits pour représenter $m$ détermine la précision numérique.

Le nombre de bits pour représenter $e$ détermine les points de dépassement et soupassement numérique (overflow/underflow).


## Quatre constantes méchanique

Pour une machine donnée, les constantes suivantes décrivent les points de dépassement et soupassement, ainsi que la précision.

|Constante|description|
|---------|-----------|
|\texttt{double.xmax}|$x>0$ le plus grand distinct de $\infty$.|
|\texttt{double.xmin}|$x>0$ le plus petit distinct de 0.|
|\texttt{double.eps}|$x>0$ le plus petit tel que $1 + x$ et 1 sont distincts.|
|\texttt{double.neg.eps}|$x>0$ le plus petit tel que $1 - x$ et 1 sont distincts.|

On appele

* \texttt{double.xmax} l'infini de la machine,
* \texttt{double.eps} l'epsilon de la machine.

## Trouver ces constantes avec R

```{r epsilon, echo=TRUE}
m = .Machine
m$double.eps
m$double.neg.eps
m$double.xmin
m$double.xmax
```

## Propogation de l'erreur

* L'erreur relative du résultat d'un calcul peut être très différente de l'erreur des intrants.
* Supposez qu'on évalue la dérivée numérique suivante, pour approximer la dérivée de la fonction $e^x$ à $x=0$ :
\[
  d_h = \frac{e^h - e^{-h}}{2h},
\]
où $h > 0$ est très petit.
* Mettons que les erreurs relatives maximales de $e^h$ et $e^{-h}$ sont $\epsilon$.
* Puisque $e^x = 1$ à $x=0$, les erreurs abolues sont pareilles.
* Par une expansion Taylor,
\[
  d_h \approx \frac{2h \pm 2\epsilon}{2h} = 1 \pm \frac{\epsilon}{h}
\]
* L'erreur relative du résultat peut être aussi grande que $\epsilon/h$.

## Expansions de Taylor et de Mercator de la fonction $\ln x$

L'expansion de Taylor de $\ln x$ autour de $x=1$ :
\[
  \ln x = \sum_{k=1}^\infty \frac{(-1)^k(x-1)^k}{k}
  = (x-1) - \frac{(x-1)^2}{2} + \frac{(x-1)^3}{3} - \ldots
\]

L'expansion de Mercator :
\[
  \ln (1+x) = \sum_{k=1}^\infty \frac{(-1)^k x^k}{k}
  = x - \frac{x^2}{2} + \frac{x^3}{3} - \ldots
\]

* Si on veux évaluer $\ln (1+x)$ quand $x$ est petit et disponsible, on ne veut pas
calculer $1+x$ comme résultat intermédiaire.

* La fonction \texttt{log1p} en R (et autres langages) évalue la fonction $f(x) = \ln(1+x)$ directement.

## La fonction \texttt{log1p}

```{r log1p}
x = seq(-2e-15, 2e-15, length.out=1000)
plot(x, log1p(x), 'l')
lines(x, log(1+x), col='red')
```

## Troncation mathématique

Une autre source d'erreur est la troncation mathématique.

La valeur exacte de la fonction exponentielle est
\[
  e^x = \sum_{n=0}^\infty \frac{x^n}{n!},
\]
mais on pratique il faut tronquer et utiliser
\[
  \sum_{n=0}^N \frac{x^n}{n!}.
\]

Le point plus général :

* Un algorithme iteratif génére une suite de valeurs $x_N$, $N=1,2,\ldots$, qui converge au résultat voulu $x^* \equiv \lim_{N\to \infty} x_N$.
* Il faut accepter une valeur approximative $x_N$, pour $N$ fini.

## Analyse (de la complexité) d'algorithmes

### Notation $O(\cdot)$

* Pour des fonctions $f$ et $g$ sur $\mathbb{N}$, on écrit $f(n) = O(g(n))$ s'il existe $M > 0$ tel que $|f(n)| \leq Mg(n)$.
* Par exemple $f(n) = 6n^2 + 8n + 2 = O(n^2)$

### La complexité de certains algorithmes

* $O(1)$ : nombre d'opérations pour trouver le $i$-ième élément dans un $n$-vecteur;
* $O(\log n)$ : nombre de comparaisons pour trouver un élément donnée dans un $n$-vecteur, par recherche binaire,
* $O(n)$ : pour trouver un élément donnée dans un $n$-vecteur, par recherche exhaustive;
* $O(n^2)$ : nombre de multiplications scalaires pour multiplier une matrice $n\times n$ et un vecteur $n\times 1$,
* $O(n^3)$ : pour multiplier deux matrices $n\times n$ (méthode évidente)
* $O(n^{2.81})$ : pour multiplier deux matrices $n\times n$ (Algorithme de Strassen)

## L'évaluation des polynomes avec la méthode de Horner

### Le problème est l'évaluation du polynôme $a_0 + a_1 x + \cdots + a_n x^n$.

### Trois méthodes

1. Évaluation naïve, $O(n^2)$ multiplications, $O(1)$ registres :
\[
  a_0 + a_1 * x + a_2 * x * x + a_3 * x * x * x + \cdots
\]
1. Meilleure, avec $2n$ opérations, $O(n)$ registres :
    a. Calculer $x^i = x^{i-1} * x$, $i=2,\ldots,n$.
    a. Calculer $a_0 + a_1 * x + \cdots + a_n * x^n$.
1. La méthode de Horner, $n$ multiplications, $O(1)$ registres :
\[
  a_0 + x * (a_1 + x * (a_2 + x * (a_3 + \cdots + x * (a_{n-1} + x * a_n)\ldots)))
\]

## Complexité et l'importance d'algorithmes d'ordre polynomial

* Soit $n$ le nombre de scalaires dans l'intrant du problème.
* L'ordre de complexité d'un problème est l'ordre de complexité du meilleur algorithme qui résoud le problème.
* Un algorithme est d'ordre polynomial si le nombre d'opérations est $O(g(n))$, pour une polynôme $g(n)$.
* Une distinction importante entre les problèmes "faisables" (pour lesquels il y a un algorithme polynomial connue pour le résoudre) et les problèmes "infaisables".
* Fonctions à sens unique et la cryptographie.
* Si un algorithme est polynomial, il est habituellement facile à prouver qu'il l'est.
    * Les polynômes sont stables pour l'addition, la multiplication et la composition :
    si $p(n)$ et $q(n)$ sont polynomiales, $p(n) + q(n)$, $p(n)q(n)$ et $p(q(n))$ le sont aussi.
    * (boucles, invocation des fonctions, etc.)

## P, NP, NP-complet, NP-difficile

- Une réduction : problème $A$ n'est pas plus difficile que $B$ ($A \leq_P B$) si $A$ peut être réduit à $B$---résolu par un algorithme pour résoudre $B$ plus un nombre d'opérations supplémentaires d'ordre polynomial.
- Classes de problèmes :
    * P : problèmes de décision (oui/no) résoluble en temps polynôme.
    * NP : problèmes de décision (oui/non) ou une réponse affirmative peut être prouvé correcte en temps polynôme.
    * NP-complet : les problèmes en NP les plus difficiles (une classe d'équivalence avec plusieurs exemples connus)
    * NP-difficile : les problèmes qui sont au moins aussi difficile que les problèmes dans NP-complet.
- P=NP? est une question non-résolue : il n'y a pas d'algorithmes polynômes connus pour résoudre les problèmes en NP.

## Quelques problèmes

\definecolor{amber}{rgb}{1.0,0.75,0.0}
(\textcolor{green}{polynomial}, \textcolor{amber}{NP-complet}, \textcolor{red}{NP-difficile})

### Problème du voyageur de commerce (Travelling Salesman)

\definecolor{amber}{rgb}{1.0,0.75,0.0}
* \textcolor{red}{Trouvez le trajet le plus court qui relie un ensemble de villes.}
* \textcolor{amber}{Y a-t-il un trajet plus court que $L$ qui relie les villes?}

### Optimisation linéaire en nombres entiers (Integer programming)

\definecolor{amber}{rgb}{1.0,0.75,0.0}
* \textcolor{red}{Trouvez la solution optimale ou montrez qu'il n'y a pas de solution.}
* \textcolor{amber}{(Cas spécial de variables binaires) Y a-t-il une solution faisable?}

### Optimisation linéaire (Linear programming)

* \textcolor{green}{Trouvez la solution optimale ou montrez qu'il n'y a pas de solution.}

## Parallelisme

Vous allez vous habituer à reconnaître deux types de problème où vous pouvez profitez des processeurs en parallèle:

1. problèmes avec l'embarras du parallelisme
1. problèmes SIMD (single instruction multiple data)

Idées en commun:
* Les tâches individuelles doivent être suffisament grandes relatif aux coûts fixes de communication.
* Les couts fixes varient beaucoup :
    - coeurs multiple d'un processeur
    - processeurs multiple d'une machine
    - machines multiples d'un cluster

### Problèmes 

## L'embarras du parallelisme

### Problèmes avec l'embarras du parallelisme

1. Évaluation d'une fonction sur une grille de points
1. Intégration numérique
1. Simulation Monte Carlo indépendent
1. Évaluation d'une fonction de log vraisemblance (souvent)
\[
  \sum_{t=1}^T \log f(y_t|\theta)
\]
1. Multiplication des matrices

### Problèmes sans l'embarras du parallelisme

1. Méthodes iteratives d'optimisation
1. Méthodes iteratives pour trouver un point fixe
1. Simulation Markov chain Monte Carlo

## SIMD (Single Instruction, Multiple Data)

* Les GPUs (processeurs graphiques) peuvent executer les mêmes instructions pour
plusieurs vecteurs différents de données.
* Convenable pour les problèmes où les boucles locales ont le même nombre d'itérations.
* Quand la structure de contrôle (control flow) est variable (if-else, do-while, etc.)
les programmes marchent mais avec gaspillage.
* Prenons encore l'évaluation d'une fonction de log vraisemblance
\[
  \sum_{t=1}^T \log f(y_t|\theta)
\]
    - Si l'évaluation de $\log f(y_t|\theta)$ utilise les mêmes instructions, peu importe la valeur de $y_t$, le problème est disposé à SIMD
    - Si la suite des instructions dépend de $y_t$, SIMD est moins intéressant.