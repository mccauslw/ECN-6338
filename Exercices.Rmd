---
title: "Exercices, ECN 6338, Hiver 2022"
author: "William McCausland"
date: "`r Sys.Date()`"
output: pdf_document
---

## Cours 1. Introduction

### Exercices théorique

1. Montrez que $6n^2 + 3n + 10 = O(n^2)$.

### Exercices de computation

1. Lisez l'aide sur la fonction \texttt{expm1} et démontrez son avantage par rapport à la fonction \texttt{exp} pour évaluer $e^x - 1$ quand $|x|$ est prés de zéro.
Vous pouvez suivre l'exemple sur \texttt{log1p} dans les diapos.

1. Téléchargez le paquet R \texttt{microbenchmark}, lisez l'aide sur la fonction \texttt{microbenchmark} et mesurer le temps nécessaire pour faire les opérations \texttt{x * y}, \texttt{x / y}, \texttt{exp(y)} et \texttt{log(x)}, pour un vecteur $x > 0$ de mille éléments et un vecteur $y$ de mille éléments.

\pagebreak

## Cours 2. La résolution de systèmes d'équations linéaires

### Exercices préliminaires

1. Soit $L_1$ et $L_2$ des matrices triangulaires inférieures $n\times n$.
Soit $U_1$ et $U_2$ des matrices triangulaires supérieures $n\times n$.
Supposez que $L_1$ et $U_1$ sont inversibles.
    a. Parmi les matrices suivantes, lequelles sont toujours triangulaires inférieures :
    $L_1L_2$, $L_1+L_2$, $L_1^{-1}$, $L_1^\top$, $U_1^\top$, $L_1U_1$?
    a. Parmi les matrices suivantes, lequelles sont toujours triangulaires supérieures :
    $U_1U_2$, $U_1+U_2$, $U_1^{-1}$, $U_1^\top$, $L_1^\top$, $L_1U_1$?
    
1. (Substitution avant et substitution arrière)
Soit $L$ et $U$ des matrices inversibles $n \times n$, où $L$ est triangulaire inférieure et $U$ est triangulaire supérieure.
Soit $y$ un vecteur $n \times 1$.
Notez que Judd utilise le terme "back substitution" pour décrire deux algorithmes distincts (a. et b.).
Plusieurs auteurs font une distinction entre "back substitution" (b.) et "forward substitution" (a.).
    a. Trouver un algorithme pour résoudre l'équation $Lx = y$.
    Notez que $L_{11} x_1 = y_1$, alors $x_1 = y_1/L_{11}$.
    a. Trouvez un algorithme pour résoudre l'équation $Ux = y$.
    Commencez par $x_n$.

1. Soit $x$ un $n$-vecteur aléatoire avec moyenne $\mu$ ($n\times 1$) et variance $\Sigma$ ($n \times n$). Soit $A$ une matrice constante $m \times n$.
Quelles sont la moyenne et la variance de $Ax$.

### Exercices théoriques

1. Quelquefois, il est plus facile de spécifier la matrice de précision $H = \Sigma^{-1}$ que la matrice de variance $\Sigma$, pour une loi gaussienne multivariée $N(\mu,\Sigma)$.
Si vous avez la matrice $H$ et non la matrice $\Sigma$, décrivez comment on peut tirer des variables aléatoires $N(\mu,\Sigma)$ et évaluer la densité gaussienne multivariée.
Commencez par la décomposition cholesky de $H$.

1. La matrice de précision $H$ pour un processus AR(1) gaussien est
tridiagonale ($H_{ij} = 0$ pour $|i-j|>1$) et symmétrique. Si $\omega = \sigma^{-2}$ est la précision de l'innovation et $\phi$ est le coefficient d'autorégression, le diagonal de $H$ est $\omega(1,1+\phi^2,\ldots,1+\phi^2,1)$ et le sous-diagonal est $-\omega(\phi, \ldots, \phi)$.
Trouvez la décomposition cholesky de $H$, en utilisant une description de l'algorithme (page 59 de Judd, par exemple).

1. Dans l'exemple MCO, calculez le vecteur $e = y - Xb$ des résiduelles et $\hat{\sigma}^2 (X^\top X)^{-1}$, une estimation de la variance de $b$. Utilisez seulement $y$ et la décomposition QR de $X$, pas $X$ elle-même. Notez que $\hat{\sigma}^2 = e^\top e/(n-k)$.

### Exercices de computation

1. Regardez la démonstration \texttt{GPA.R} de l'analyse d'une régression linéaire, exemple 3.1 dans Wooldridge (2016) Introductory Econometrics.
Calculez $b$, $e$ et $\hat{\sigma}^2 (X^\top X)^{-1}$ en utilisant la décomposition QR.
La fonction \texttt{qr} effectue la décomposition QR, les fonctions \texttt{qr.Q} et \texttt{qr.R} extraient les matrices $Q_1$ et $R_1$ du résultat.
Les fonctions \texttt{backsolve} et \texttt{t} (transpose) pourraient être utiles.

\pagebreak

## Cours 3. Quelques sujets préalables

### Exercices préliminaires

1. Trouvez le panier de consommation $(x_1,x_2)$ qui maximise $U(x_1,x_2) = x_1^{1/2} x_2^{1/2}$ sous les contraintes $p_1x_1 + p_2x_2 = m$, $x_1 \geq 0$ et $x_2 \geq 0$, où $m \geq 0$, $p_1 \geq 0$ et $p_2 \geq 0$.
Utilisez la méthode de Lagrange.

1. Si $X_i|\lambda \sim \mathrm{iid}\;\mathrm{Exp}(\lambda)$ (loi exponentielle) et $\lambda \sim \mathrm{Ga}(\alpha, \beta)$ (loi gamma)
écrivez
    a. la fonction de densité conditionnelle de $(X_1,\ldots,X_n)$ sachant $\lambda$.
    a. la densité conjointe de $(X_1,\ldots,X_n)$ et $\lambda$.

### Exercices théoriques

### Exercices de computation

Aucune, étant donnée la nature du cours 3.

\pagebreak

## Cours 4. L'optimisation statique
