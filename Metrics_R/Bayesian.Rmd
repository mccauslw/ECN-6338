---
title: "Bayesian analysis"
output: html_document
---

This vignette is an introduction to Bayesian analysis, through examples and exercises.
The vignette maximum likelihood is recommended prepartion.

### Frequentist versus Bayesian analysis

We saw in the maximum likelihood vignette that frequentist analysis is *ex ante*, in the sense that the properties of the procedure used to perform inference can be analyzed before any data is observed.
A parametric model might give the distribution of $X = (X_1,\ldots,X_n)$ up to the value of a parameter $\theta \in \Theta$; the true value $\theta_0$ of the parameter is treated as a fixed (non-random) but unknown constant.
A procedure to estimate a parameter as a function of observed data is described as a function $\hat\theta(X)$ mapping the random vector $X$ of observables into a value that one wants to be close to the true value $\theta_0$.
$\hat\theta(X)$ has a distribution that is knowable in principle (and sometimes in practice, for a small set of toy problems) up to the unknown parameter $\theta$ governing the distribution of $X$.
Properties of $\hat\theta(X)$ can be derived before any realization of $X$ is observed.
As a practical matter, the distribution of $\hat\theta(X)$ is intractable and people resort to asymptotic approximations.

In a Bayesian analysis, one treats the parameter $\theta$ as a random variable, and its (marginal) distribution represents uncertainty we have about its value before we observe the data $(x_1,\ldots,x_n)$, a single realization of the random vector $X$.
This marginal distribution of $\theta$ is called the *prior* distribution and must be specified in order to do inference.

### Bayes' rule and the elements of Bayesian analysis

Two different ways of expressing the joint density of $\theta$ and $X$ appear on the two sides of this equation:
\[
  f(\theta) f(X|\theta) = f(X) f(\theta|X).
\]
The left hand side is what the Bayesian statistician specifies: the prior density $f(\theta)$ and the data generating process $f(X|\theta)$, up to the value of the unknown parameter $\theta$; a frequentist statistician only specifies $f(X|\theta)$.
Once specified, the left hand side gives the joint density of $\theta$ and $X$ through a marginal-conditional decomposition.
In principle, the right hand side densities can also be computed, although this is much easier said than done.
The marginal density $f(X)$ is a kind of prediction of the data averaging over all parameter values according to their prior plausibility, as measured by $f(\theta)$:
\[
  f(X) = \int f(\theta) f(X|\theta)\, d\theta.
\]
When we plug in the observed data $x$, a realization of $X$, we obtain a number $f(x)$ called the marginal likelihood.
Below, we will discuss how this can be interpreted as the out-of-sample prediction record of the model for the observed data.
The density $f(\theta|X)$ is called the *posterior* distribution of $\theta$, and represents uncertainty we have about the value of $\theta$ after we observe the data; once we observe the realization $x$ of $X$, we obtain a function $f(\theta|x)$ which summarizes what we know about $\theta$ in light of the data.
The formula for $f(\theta|x)$ is given by Bayes' rule, a rearrangement of the two marginal-condition decompositions of the joint density $f(\theta,x)$ above:
\[
  f(\theta|x) = \frac{f(\theta) f(x|\theta)}{f(x)}.\quad\mbox{(Bayes' rule)}
\]


### A coin-tossing example

We revisit the coin-tossing example, where $X_i$ is Bernoulli with parameter $\theta$.
Let $x\equiv (x_1,\ldots,x_n)$ be a realization of $n$ coin-tosses.
We saw that the likelihood function is
\[
  {\cal L}(\theta;x) = \theta^{n_1} (1-\theta)^{n_0},
\]
where $n_1 = \sum_{i=1}^n x_i$ is the number of times $x_i = 1$ is observed and $n_0 = n-n_1$ is the number of times $x_i = 0$ is observed.

A commonly used parametric distribution on the interval $\Theta = [0,1]$ is the [beta distribution](https://en.wikipedia.org/wiki/Beta_distribution).
The beta density has the convenient property that it has the same functional form as the likelihood function:
\[
  f(\theta|\alpha,\beta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} \theta^{\alpha-1} (1-\theta)^{\beta-1},
\]
where $\Gamma(\cdot)$ is the [gamma function](https://en.wikipedia.org/wiki/Gamma_function), and $\alpha,\beta>0$ are *hyper-parameters*---parameters of the prior distribution.
We know this density integrates to one; it is like finding the following entry in a table of integrals:
\[
  \int_0^1 x^{\alpha-1} (1-x)^{\beta-1} = \frac{\Gamma(\alpha) \Gamma(\beta)}{\Gamma(\alpha + \beta)},
  \qquad \alpha, \beta > 0.
\]

If we want to use a beta distribution as our prior distribution of $\theta$, we just need to select values of $\alpha$ and $\beta$; choosing $\alpha = \beta = 1$ gives the uniform distribution on $[0,1]$.
If $\theta \sim \mathrm{Be}(\alpha,\beta)$, the joint density of $\theta$ and $X$ is
\[
  f(\theta,x) = f(\theta) f(x|\theta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} \theta^{\alpha-1} (1-\theta)^{\beta-1}
  \cdot \theta^{n_1} (1-\theta)^{n_0}
  = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} \theta^{\alpha + n_1 -1} (1-\theta)^{\beta + n_0 -1}.
\]

Looking back at the equation "Bayes' rule", we can see that since $f(x)$ is not a function of $\theta$, $f(\theta|x)$ is proportional to $f(\theta)f(x|\theta)$.
We conclude that $f(\theta|x) \propto \theta^{\alpha + n_1 -1} (1-\theta)^{\beta + n_0 -1}$.
This expression is in turn proportional to a beta density with values $\alpha + n_1$ and $\beta + n_0$ of the two shape parameters.
The only density integrating to one and proportional to $\theta^{\alpha + n_1 -1} (1-\theta)^{\beta + n_0 -1}$ is
\[
  f(\theta|x) = \frac{\Gamma(\alpha + \beta + n)}{\Gamma(\alpha + n_1) \Gamma(\beta + n_0)} \theta^{\alpha + n_1 -1} (1-\theta)^{\beta + n_0 -1}.
\]
Our uncertainty about beta changes when we observe the data $x$.
Before, it is represented by a $\mathrm{Be}(\alpha, \beta)$; after, by a $\mathrm{Be}(\alpha + n_1, \beta + n_0)$.

Suppose we have a sample from a population of unemployed people, and we observe whether they are still unemployed after a year.
We can express our prior uncertainty about the probability $\theta$ of an unemployment spell ending using a beta distribution with parameters $\alpha = 8$ and $\beta = 2$.
Then we observe that $n_1 = 57$ people found employment and $n_0 = 16$ people did not.
The prior and posterior densities are shown in this graphic:
```{r beta_prior_post, echo=TRUE}
alpha = 8; beta = 2; n_1 = 57; n_0 = 16
theta = seq(0, 1, by=0.001)
plot(theta, dbeta(theta, alpha + n_1, beta + n_0), type='l',
     main='Prior (dashed) and posterior (solid) densities of theta', xlab='theta', ylab='density')
lines(theta, dbeta(theta, alpha, beta), lt='dashed')
```

